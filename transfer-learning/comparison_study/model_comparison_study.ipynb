{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb4db6a",
   "metadata": {},
   "source": [
    "\n",
    "# Alzheimer's MRI Classification — Model Comparison Study\n",
    "\n",
    "This notebook auto-discovers metric files, loads their training and evaluation statistics, and produces:\n",
    "\n",
    "- A consolidated **summary table** of key metrics across models  \n",
    "- **Training curves** (accuracy, loss, validation accuracy, validation loss)  \n",
    "- **Learning-rate** schedules  \n",
    "- **Per-class** precision/recall/F1 comparisons across models  \n",
    "- **Confusion matrices** for each model  \n",
    "- **Prediction distribution vs. class support** checks for imbalance/over/under-prediction  \n",
    "- A **best-by-metric** report (e.g., top test accuracy, top macro F1, etc.)  \n",
    "\n",
    "> Tip: Drop any number of `*_metrics.json` files in the `metrics/` folder and re-run all cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc34e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# Notebook parameters\n",
    "METRICS_GLOB = \"metrics/*_metrics.json\"\n",
    "\n",
    "def load_metrics_files(pattern: str = METRICS_GLOB) -> List[Dict[str, Any]]:\n",
    "    paths = sorted(glob.glob(pattern))\n",
    "    data = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                d = json.load(f)\n",
    "            d[\"_file\"] = p\n",
    "            data.append(d)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load {p}: {e}\")\n",
    "    if not data:\n",
    "        print(f\"[WARN] No files matched {pattern}. Place *_metrics.json files under metrics/.\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(data)} file(s):\")\n",
    "        for d in data:\n",
    "            print(\" -\", d.get('_file'))\n",
    "    return data\n",
    "\n",
    "metrics_data = load_metrics_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1559f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Peek keys (first file) for sanity\n",
    "if metrics_data:\n",
    "    first = metrics_data[0]\n",
    "    print(\"Example model:\", first.get(\"model_name\"))\n",
    "    print(\"Top-level keys:\", list(first.keys()))\n",
    "    print(\"Training history keys:\", list(first.get(\"training_history\", {}).keys()))\n",
    "    print(\"Evaluation keys:\", list(first.get(\"evaluation_metrics\", {}).keys()))\n",
    "    print(\"Training info:\", first.get(\"training_info\", {}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3514665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_summary_rows(data: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for d in data:\n",
    "        name = d.get(\"model_name\") or d.get(\"evaluation_metrics\", {}).get(\"model_name\") or os.path.basename(d.get(\"_file\", \"unknown\"))\n",
    "        ev = d.get(\"evaluation_metrics\", {})\n",
    "        cr = ev.get(\"classification_report_dict\", {}) or {}\n",
    "        macro = cr.get(\"macro avg\", {})\n",
    "        weighted = cr.get(\"weighted avg\", {})\n",
    "        acc = ev.get(\"test_accuracy\", cr.get(\"accuracy\"))\n",
    "\n",
    "        tr_info = d.get(\"training_info\", {})\n",
    "        total_epochs = tr_info.get(\"total_epochs\")\n",
    "        best_val_acc = tr_info.get(\"best_val_accuracy\")\n",
    "        best_val_loss = tr_info.get(\"best_val_loss\")\n",
    "        final_train_acc = tr_info.get(\"final_train_accuracy\")\n",
    "        final_train_loss = tr_info.get(\"final_train_loss\")\n",
    "\n",
    "        rows.append({\n",
    "            \"model\": name,\n",
    "            \"test_accuracy\": acc,\n",
    "            \"macro_precision\": macro.get(\"precision\"),\n",
    "            \"macro_recall\": macro.get(\"recall\"),\n",
    "            \"macro_f1\": macro.get(\"f1-score\"),\n",
    "            \"weighted_precision\": weighted.get(\"precision\"),\n",
    "            \"weighted_recall\": weighted.get(\"recall\"),\n",
    "            \"weighted_f1\": weighted.get(\"f1-score\"),\n",
    "            \"num_test_samples\": ev.get(\"num_test_samples\"),\n",
    "            \"total_epochs\": total_epochs,\n",
    "            \"best_val_accuracy\": best_val_acc,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"final_train_accuracy\": final_train_acc,\n",
    "            \"final_train_loss\": final_train_loss,\n",
    "            \"_file\": d.get(\"_file\")\n",
    "        })\n",
    "    df = pd.DataFrame(rows).sort_values(by=[\"test_accuracy\",\"macro_f1\"], ascending=[False, False])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "summary_df = extract_summary_rows(metrics_data) if metrics_data else pd.DataFrame()\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d8cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not summary_df.empty:\n",
    "    out_csv = \"model_comparison_summary.csv\"\n",
    "    summary_df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved summary to {out_csv}\")\n",
    "else:\n",
    "    print(\"No summary to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b712ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_best(df: pd.DataFrame, metric: str) -> pd.DataFrame:\n",
    "    if metric not in df.columns or df.empty:\n",
    "        return pd.DataFrame()\n",
    "    top = df.sort_values(metric, ascending=False).head(3).reset_index(drop=True)\n",
    "    return top[[\"model\", metric, \"_file\"]]\n",
    "\n",
    "if not summary_df.empty:\n",
    "    metrics_to_rank = [\"test_accuracy\", \"macro_f1\", \"macro_precision\", \"macro_recall\", \"weighted_f1\"]\n",
    "    for m in metrics_to_rank:\n",
    "        print(f\"\\nTop-3 by {m}:\")\n",
    "        display(get_best(summary_df, m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f789b27",
   "metadata": {},
   "source": [
    "## Training & Validation Curves (Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_curve(data, key: str, title: str, ylabel: str):\n",
    "    if not data:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for d in data:\n",
    "        name = d.get(\"model_name\") or os.path.basename(d.get(\"_file\", \"unknown\"))\n",
    "        y = d.get(\"training_history\", {}).get(key, [])\n",
    "        if y:\n",
    "            plt.plot(range(1, len(y)+1), y, label=name)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_combined_curve(metrics_data, \"accuracy\", \"Training Accuracy (per epoch)\", \"accuracy\")\n",
    "plot_combined_curve(metrics_data, \"loss\", \"Training Loss (per epoch)\", \"loss\")\n",
    "plot_combined_curve(metrics_data, \"val_accuracy\", \"Validation Accuracy (per epoch)\", \"val_accuracy\")\n",
    "plot_combined_curve(metrics_data, \"val_loss\", \"Validation Loss (per epoch)\", \"val_loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409e6eb",
   "metadata": {},
   "source": [
    "## Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e952f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_combined_curve(metrics_data, \"learning_rate\", \"Learning Rate (per epoch)\", \"learning_rate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291df2b",
   "metadata": {},
   "source": [
    "## Per-class Metrics — Cross-model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1892e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_per_class_metric(data, metric_key: str) -> pd.DataFrame:\n",
    "    r\"\"\"\n",
    "    Returns a DataFrame indexed by class with columns for each model's metric.\n",
    "    metric_key in {\"precision\",\"recall\",\"f1-score\"}\n",
    "    r\"\"\"\n",
    "    rows = {}\n",
    "    classes = set()\n",
    "    models = []\n",
    "    for d in data:\n",
    "        name = d.get(\"model_name\") or os.path.basename(d.get(\"_file\",\"unknown\"))\n",
    "        models.append(name)\n",
    "        cr = d.get(\"evaluation_metrics\",{}).get(\"classification_report_dict\",{})\n",
    "        for cls, vals in cr.items():\n",
    "            if cls in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                continue\n",
    "            classes.add(cls)\n",
    "    classes = sorted(list(classes))\n",
    "    df = pd.DataFrame(index=classes, columns=models, dtype=float)\n",
    "    for d in data:\n",
    "        name = d.get(\"model_name\") or os.path.basename(d.get(\"_file\",\"unknown\"))\n",
    "        cr = d.get(\"evaluation_metrics\",{}).get(\"classification_report_dict\",{})\n",
    "        for cls in classes:\n",
    "            val = cr.get(cls,{}).get(metric_key, np.nan)\n",
    "            df.loc[cls, name] = val\n",
    "    return df\n",
    "\n",
    "if metrics_data:\n",
    "    for m in [\"precision\",\"recall\",\"f1-score\"]:\n",
    "        df_metric = collect_per_class_metric(metrics_data, m)\n",
    "        display(df_metric)\n",
    "        # One plot per metric (no subplots)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        ax = df_metric.plot(kind=\"bar\", ax=plt.gca())\n",
    "        plt.title(f\"{m.capitalize()} by Class — All Models\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(m)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.legend(title=\"Model\", loc=\"best\", bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No metrics data available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17ec36",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm, class_names, title):\n",
    "    plt.figure()\n",
    "    cm = np.array(cm)\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    # Annotate counts\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for d in metrics_data:\n",
    "    ev = d.get(\"evaluation_metrics\", {})\n",
    "    cm = ev.get(\"confusion_matrix\")\n",
    "    names = ev.get(\"class_names\", [])\n",
    "    title = f\"Confusion Matrix — {d.get('model_name', os.path.basename(d.get('_file','unknown')))}\"\n",
    "    if cm is not None and names:\n",
    "        plot_confusion_matrix(cm, names, title)\n",
    "    else:\n",
    "        print(f\"[WARN] Missing confusion matrix for {d.get('model_name', d.get('_file'))}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15820749",
   "metadata": {},
   "source": [
    "## Prediction Distribution vs. True Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_pred_vs_support(d):\n",
    "    name = d.get(\"model_name\") or os.path.basename(d.get(\"_file\",\"unknown\"))\n",
    "    ev = d.get(\"evaluation_metrics\", {})\n",
    "    pred_dist = ev.get(\"prediction_distribution\", {})\n",
    "    cr = ev.get(\"classification_report_dict\", {})\n",
    "    # Build support dict from classification report (per-class entries only)\n",
    "    true_support = {}\n",
    "    for cls, vals in cr.items():\n",
    "        if cls in [\"accuracy\",\"macro avg\",\"weighted avg\"]:\n",
    "            continue\n",
    "        supp = vals.get(\"support\")\n",
    "        if supp is not None:\n",
    "            true_support[cls] = supp\n",
    "\n",
    "    # Align keys\n",
    "    all_classes = sorted(set(list(pred_dist.keys()) + list(true_support.keys())))\n",
    "    pred_vals = [pred_dist.get(k, 0) for k in all_classes]\n",
    "    supp_vals = [true_support.get(k, 0) for k in all_classes]\n",
    "\n",
    "    df = pd.DataFrame({\"predicted_count\": pred_vals, \"true_support\": supp_vals}, index=all_classes)\n",
    "    display(df)\n",
    "\n",
    "    # One plot per model (no subplots)\n",
    "    plt.figure()\n",
    "    df.plot(kind=\"bar\")\n",
    "    plt.title(f\"Predicted vs True Support — {name}\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for d in metrics_data:\n",
    "    plot_pred_vs_support(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bca119",
   "metadata": {},
   "source": [
    "## Error Analysis Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If we save misclassification indices or per-sample predictions elsewhere,\n",
    "# we could extend this section to load them and drill down into failure modes.\n",
    "# Placeholders for future extensions:\n",
    "# - Per-class confusion breakdown with normalized rates\n",
    "# - Threshold sweeps (if probabilities are available)\n",
    "# - Calibration plots (if probabilities are available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4e17a",
   "metadata": {},
   "source": [
    "## Appendix — Raw Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for d in metrics_data:\n",
    "    name = d.get(\"model_name\") or os.path.basename(d.get(\"_file\",\"unknown\"))\n",
    "    txt = d.get(\"evaluation_metrics\", {}).get(\"classification_report_str\")\n",
    "    if txt:\n",
    "        print(f\"\\n===== {name} =====\")\n",
    "        print(txt)\n",
    "    else:\n",
    "        print(f\"[WARN] No classification_report_str for {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d690006",
   "metadata": {},
   "source": [
    "##  Model Ranking using Rule Based `Composite Score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd20960d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>model</th>\n",
       "      <th>composite_score</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>best_val_accuracy</th>\n",
       "      <th>final_train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ViT-B8-fe</td>\n",
       "      <td>0.987020</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.752358</td>\n",
       "      <td>0.749872</td>\n",
       "      <td>0.743671</td>\n",
       "      <td>0.833938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ViT-L16-fe</td>\n",
       "      <td>0.980790</td>\n",
       "      <td>0.746835</td>\n",
       "      <td>0.753696</td>\n",
       "      <td>0.751085</td>\n",
       "      <td>0.772152</td>\n",
       "      <td>0.813975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Xception</td>\n",
       "      <td>0.976374</td>\n",
       "      <td>0.753165</td>\n",
       "      <td>0.753360</td>\n",
       "      <td>0.751957</td>\n",
       "      <td>0.715190</td>\n",
       "      <td>0.780399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ViT-B16-fe</td>\n",
       "      <td>0.965009</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.737052</td>\n",
       "      <td>0.734672</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.882033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ViT-B32-fe</td>\n",
       "      <td>0.965001</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.734399</td>\n",
       "      <td>0.731818</td>\n",
       "      <td>0.756329</td>\n",
       "      <td>0.924682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>ViT-S16-fe</td>\n",
       "      <td>0.964437</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.735502</td>\n",
       "      <td>0.733081</td>\n",
       "      <td>0.737342</td>\n",
       "      <td>0.924682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>ViT-b16</td>\n",
       "      <td>0.936419</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>0.720200</td>\n",
       "      <td>0.718137</td>\n",
       "      <td>0.718354</td>\n",
       "      <td>0.812160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>NASNetLarge</td>\n",
       "      <td>0.935198</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>0.726297</td>\n",
       "      <td>0.724275</td>\n",
       "      <td>0.689873</td>\n",
       "      <td>0.820327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>MobileNetV2</td>\n",
       "      <td>0.918438</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.713932</td>\n",
       "      <td>0.711432</td>\n",
       "      <td>0.731013</td>\n",
       "      <td>0.725953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>0.905542</td>\n",
       "      <td>0.715190</td>\n",
       "      <td>0.707012</td>\n",
       "      <td>0.703628</td>\n",
       "      <td>0.718354</td>\n",
       "      <td>0.709619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>ViT-R26-S32-medaug-fe</td>\n",
       "      <td>0.890636</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.689453</td>\n",
       "      <td>0.686756</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.810345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>ViT-R26-S32-lightaug-fe</td>\n",
       "      <td>0.886843</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.693836</td>\n",
       "      <td>0.690804</td>\n",
       "      <td>0.693038</td>\n",
       "      <td>0.843013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>InceptionV3</td>\n",
       "      <td>0.866286</td>\n",
       "      <td>0.689873</td>\n",
       "      <td>0.681516</td>\n",
       "      <td>0.679700</td>\n",
       "      <td>0.677215</td>\n",
       "      <td>0.761343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>DenseNet121</td>\n",
       "      <td>0.848188</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.679787</td>\n",
       "      <td>0.677844</td>\n",
       "      <td>0.686709</td>\n",
       "      <td>0.589837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>InceptionResNetV2</td>\n",
       "      <td>0.751832</td>\n",
       "      <td>0.626582</td>\n",
       "      <td>0.614589</td>\n",
       "      <td>0.612755</td>\n",
       "      <td>0.617089</td>\n",
       "      <td>0.654265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>VGG16</td>\n",
       "      <td>0.731798</td>\n",
       "      <td>0.613924</td>\n",
       "      <td>0.615729</td>\n",
       "      <td>0.614196</td>\n",
       "      <td>0.604430</td>\n",
       "      <td>0.537205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>VGG19</td>\n",
       "      <td>0.638897</td>\n",
       "      <td>0.569620</td>\n",
       "      <td>0.541300</td>\n",
       "      <td>0.539491</td>\n",
       "      <td>0.579114</td>\n",
       "      <td>0.485481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>ResNet101</td>\n",
       "      <td>0.384372</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.392094</td>\n",
       "      <td>0.392676</td>\n",
       "      <td>0.462025</td>\n",
       "      <td>0.421960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>ResNet50</td>\n",
       "      <td>0.302896</td>\n",
       "      <td>0.373418</td>\n",
       "      <td>0.323307</td>\n",
       "      <td>0.322979</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.396552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>EfficientNetB0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.102289</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.216878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank                    model  composite_score  test_accuracy  macro_f1  weighted_f1  best_val_accuracy  final_train_accuracy\n",
       "0      1                ViT-B8-fe         0.987020       0.759494  0.752358     0.749872           0.743671              0.833938\n",
       "1      2               ViT-L16-fe         0.980790       0.746835  0.753696     0.751085           0.772152              0.813975\n",
       "2      3                 Xception         0.976374       0.753165  0.753360     0.751957           0.715190              0.780399\n",
       "3      4               ViT-B16-fe         0.965009       0.740506  0.737052     0.734672           0.759494              0.882033\n",
       "4      5               ViT-B32-fe         0.965001       0.740506  0.734399     0.731818           0.756329              0.924682\n",
       "5      6               ViT-S16-fe         0.964437       0.740506  0.735502     0.733081           0.737342              0.924682\n",
       "6      7                  ViT-b16         0.936419       0.727848  0.720200     0.718137           0.718354              0.812160\n",
       "7      8              NASNetLarge         0.935198       0.727848  0.726297     0.724275           0.689873              0.820327\n",
       "8      9              MobileNetV2         0.918438       0.721519  0.713932     0.711432           0.731013              0.725953\n",
       "9     10              DenseNet201         0.905542       0.715190  0.707012     0.703628           0.718354              0.709619\n",
       "10    11    ViT-R26-S32-medaug-fe         0.890636       0.708861  0.689453     0.686756           0.667722              0.810345\n",
       "11    12  ViT-R26-S32-lightaug-fe         0.886843       0.696203  0.693836     0.690804           0.693038              0.843013\n",
       "12    13              InceptionV3         0.866286       0.689873  0.681516     0.679700           0.677215              0.761343\n",
       "13    14              DenseNet121         0.848188       0.683544  0.679787     0.677844           0.686709              0.589837\n",
       "14    15        InceptionResNetV2         0.751832       0.626582  0.614589     0.612755           0.617089              0.654265\n",
       "15    16                    VGG16         0.731798       0.613924  0.615729     0.614196           0.604430              0.537205\n",
       "16    17                    VGG19         0.638897       0.569620  0.541300     0.539491           0.579114              0.485481\n",
       "17    18                ResNet101         0.384372       0.405063  0.392094     0.392676           0.462025              0.421960\n",
       "18    19                 ResNet50         0.302896       0.373418  0.323307     0.322979           0.443038              0.396552\n",
       "19    20           EfficientNetB0         0.000000       0.253165  0.101010     0.102289           0.253165              0.216878"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def rank_models(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        print(\"No summary_df available to rank.\")\n",
    "        return df\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Select metrics to rank on\n",
    "    metrics_for_score = [\n",
    "        \"test_accuracy\",\n",
    "        \"macro_precision\", \"macro_recall\", \"macro_f1\",\n",
    "        \"weighted_precision\", \"weighted_recall\", \"weighted_f1\",\n",
    "        \"best_val_accuracy\",\n",
    "        \"final_train_accuracy\"\n",
    "    ]\n",
    "    \n",
    "    # Scale values 0–1 for fair comparison (ignore NaNs)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(df_copy[metrics_for_score].fillna(0))\n",
    "    df_scaled = pd.DataFrame(scaled, columns=[m+\"_scaled\" for m in metrics_for_score])\n",
    "\n",
    "    # Composite score (weights can be adjusted)\n",
    "    # More weight to test accuracy and macro_f1\n",
    "    weights = {\n",
    "        \"test_accuracy_scaled\": 0.3,\n",
    "        \"macro_f1_scaled\": 0.2,\n",
    "        \"macro_precision_scaled\": 0.1,\n",
    "        \"macro_recall_scaled\": 0.1,\n",
    "        \"weighted_f1_scaled\": 0.1,\n",
    "        \"weighted_precision_scaled\": 0.05,\n",
    "        \"weighted_recall_scaled\": 0.05,\n",
    "        \"best_val_accuracy_scaled\": 0.05,\n",
    "        \"final_train_accuracy_scaled\": 0.05\n",
    "    }\n",
    "\n",
    "    df_copy[\"composite_score\"] = 0\n",
    "    for col, w in weights.items():\n",
    "        df_copy[\"composite_score\"] += df_scaled[col] * w\n",
    "\n",
    "    # Sort by composite score\n",
    "    ranked = df_copy.sort_values(by=\"composite_score\", ascending=False).reset_index(drop=True)\n",
    "    ranked.insert(0, \"rank\", ranked.index + 1)\n",
    "    \n",
    "    # Show key metrics + composite score\n",
    "    return ranked[\n",
    "        [\"rank\", \"model\", \"composite_score\", \"test_accuracy\", \"macro_f1\", \"weighted_f1\", \n",
    "         \"best_val_accuracy\", \"final_train_accuracy\"]\n",
    "    ]\n",
    "\n",
    "ranked_models = rank_models(summary_df)\n",
    "display(ranked_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44d2c0",
   "metadata": {},
   "source": [
    "##  Model Ranking using PCA Based `Composite Score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6568befd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>composite_score</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>final_train_accuracy</th>\n",
       "      <th>best_val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ViT-B32-fe</td>\n",
       "      <td>3.363279</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.734399</td>\n",
       "      <td>0.924682</td>\n",
       "      <td>0.756329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ViT-B16-fe</td>\n",
       "      <td>3.338243</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.737052</td>\n",
       "      <td>0.882033</td>\n",
       "      <td>0.759494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ViT-S16-fe</td>\n",
       "      <td>3.297701</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.735502</td>\n",
       "      <td>0.924682</td>\n",
       "      <td>0.737342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ViT-B8-fe</td>\n",
       "      <td>3.131703</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.752358</td>\n",
       "      <td>0.833938</td>\n",
       "      <td>0.743671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ViT-L16-fe</td>\n",
       "      <td>2.799830</td>\n",
       "      <td>0.746835</td>\n",
       "      <td>0.753696</td>\n",
       "      <td>0.813975</td>\n",
       "      <td>0.772152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ViT-b16</td>\n",
       "      <td>2.465443</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>0.720200</td>\n",
       "      <td>0.812160</td>\n",
       "      <td>0.718354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Xception</td>\n",
       "      <td>2.354463</td>\n",
       "      <td>0.753165</td>\n",
       "      <td>0.753360</td>\n",
       "      <td>0.780399</td>\n",
       "      <td>0.715190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NASNetLarge</td>\n",
       "      <td>2.343322</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>0.726297</td>\n",
       "      <td>0.820327</td>\n",
       "      <td>0.689873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ViT-R26-S32-lightaug-fe</td>\n",
       "      <td>1.868207</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.693836</td>\n",
       "      <td>0.843013</td>\n",
       "      <td>0.693038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ViT-R26-S32-medaug-fe</td>\n",
       "      <td>1.652555</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.689453</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.667722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MobileNetV2</td>\n",
       "      <td>1.316404</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.713932</td>\n",
       "      <td>0.725953</td>\n",
       "      <td>0.731013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>InceptionV3</td>\n",
       "      <td>1.300099</td>\n",
       "      <td>0.689873</td>\n",
       "      <td>0.681516</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.677215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>1.211294</td>\n",
       "      <td>0.715190</td>\n",
       "      <td>0.707012</td>\n",
       "      <td>0.709619</td>\n",
       "      <td>0.718354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DenseNet121</td>\n",
       "      <td>-0.380562</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.679787</td>\n",
       "      <td>0.589837</td>\n",
       "      <td>0.686709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>InceptionResNetV2</td>\n",
       "      <td>-0.760149</td>\n",
       "      <td>0.626582</td>\n",
       "      <td>0.614589</td>\n",
       "      <td>0.654265</td>\n",
       "      <td>0.617089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VGG16</td>\n",
       "      <td>-2.295932</td>\n",
       "      <td>0.613924</td>\n",
       "      <td>0.615729</td>\n",
       "      <td>0.537205</td>\n",
       "      <td>0.604430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VGG19</td>\n",
       "      <td>-3.419928</td>\n",
       "      <td>0.569620</td>\n",
       "      <td>0.541300</td>\n",
       "      <td>0.485481</td>\n",
       "      <td>0.579114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ResNet101</td>\n",
       "      <td>-6.073740</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.392094</td>\n",
       "      <td>0.421960</td>\n",
       "      <td>0.462025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ResNet50</td>\n",
       "      <td>-6.983505</td>\n",
       "      <td>0.373418</td>\n",
       "      <td>0.323307</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.443038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>EfficientNetB0</td>\n",
       "      <td>-10.528727</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.216878</td>\n",
       "      <td>0.253165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  composite_score  test_accuracy  macro_f1  final_train_accuracy  best_val_accuracy\n",
       "0                ViT-B32-fe         3.363279       0.740506  0.734399              0.924682           0.756329\n",
       "1                ViT-B16-fe         3.338243       0.740506  0.737052              0.882033           0.759494\n",
       "2                ViT-S16-fe         3.297701       0.740506  0.735502              0.924682           0.737342\n",
       "3                 ViT-B8-fe         3.131703       0.759494  0.752358              0.833938           0.743671\n",
       "4                ViT-L16-fe         2.799830       0.746835  0.753696              0.813975           0.772152\n",
       "5                   ViT-b16         2.465443       0.727848  0.720200              0.812160           0.718354\n",
       "6                  Xception         2.354463       0.753165  0.753360              0.780399           0.715190\n",
       "7               NASNetLarge         2.343322       0.727848  0.726297              0.820327           0.689873\n",
       "8   ViT-R26-S32-lightaug-fe         1.868207       0.696203  0.693836              0.843013           0.693038\n",
       "9     ViT-R26-S32-medaug-fe         1.652555       0.708861  0.689453              0.810345           0.667722\n",
       "10              MobileNetV2         1.316404       0.721519  0.713932              0.725953           0.731013\n",
       "11              InceptionV3         1.300099       0.689873  0.681516              0.761343           0.677215\n",
       "12              DenseNet201         1.211294       0.715190  0.707012              0.709619           0.718354\n",
       "13              DenseNet121        -0.380562       0.683544  0.679787              0.589837           0.686709\n",
       "14        InceptionResNetV2        -0.760149       0.626582  0.614589              0.654265           0.617089\n",
       "15                    VGG16        -2.295932       0.613924  0.615729              0.537205           0.604430\n",
       "16                    VGG19        -3.419928       0.569620  0.541300              0.485481           0.579114\n",
       "17                ResNet101        -6.073740       0.405063  0.392094              0.421960           0.462025\n",
       "18                 ResNet50        -6.983505       0.373418  0.323307              0.396552           0.443038\n",
       "19           EfficientNetB0       -10.528727       0.253165  0.101010              0.216878           0.253165"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to load all JSON metric files and extract summary info\n",
    "def load_metrics(json_folder='metrics/'):\n",
    "    files = glob.glob(os.path.join(json_folder, '*_metrics.json'))\n",
    "    rows = []\n",
    "    for f in files:\n",
    "        data = json.load(open(f))\n",
    "        ev = data.get('evaluation_metrics', {})\n",
    "        cr = ev.get('classification_report_dict', {})\n",
    "        macro = cr.get('macro avg', {})\n",
    "        weighted = cr.get('weighted avg', {})\n",
    "        tr_info = data.get('training_info', {})\n",
    "        training_history = data.get('training_history', {})\n",
    "        rows.append({\n",
    "            'model': data.get('model_name', os.path.basename(f)),\n",
    "            'test_accuracy': ev.get('test_accuracy'),\n",
    "            'macro_precision': macro.get('precision'),\n",
    "            'macro_recall': macro.get('recall'),\n",
    "            'macro_f1': macro.get('f1-score'),\n",
    "            'weighted_precision': weighted.get('precision'),\n",
    "            'weighted_recall': weighted.get('recall'),\n",
    "            'weighted_f1': weighted.get('f1-score'),\n",
    "            'best_val_accuracy': tr_info.get('best_val_accuracy'),\n",
    "            'final_train_accuracy': tr_info.get('final_train_accuracy'),\n",
    "            'training_history': training_history,\n",
    "            '_file': f\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Extract engineered features from training history dict\n",
    "def extract_history_features(history_dict):\n",
    "    def auc(arr): \n",
    "        if arr is None or len(arr) == 0:\n",
    "            return 0.0\n",
    "        return np.sum(arr)\n",
    "    \n",
    "    features = {}\n",
    "    features['train_acc_auc'] = auc(history_dict.get('accuracy', []))\n",
    "    features['val_acc_auc'] = auc(history_dict.get('val_accuracy', []))\n",
    "    features['train_loss_auc'] = auc(history_dict.get('loss', []))\n",
    "    features['val_loss_auc'] = auc(history_dict.get('val_loss', []))\n",
    "\n",
    "    features['final_train_acc'] = history_dict.get('accuracy', [0])[-1] if len(history_dict.get('accuracy', [])) > 0 else 0.0\n",
    "    features['final_val_acc'] = history_dict.get('val_accuracy', [0])[-1] if len(history_dict.get('val_accuracy', [])) > 0 else 0.0\n",
    "    features['final_train_loss'] = history_dict.get('loss', [0])[-1] if len(history_dict.get('loss', [])) > 0 else 0.0\n",
    "    features['final_val_loss'] = history_dict.get('val_loss', [0])[-1] if len(history_dict.get('val_loss', [])) > 0 else 0.0\n",
    "\n",
    "    # Overfitting indicators\n",
    "    features['val_train_acc_diff'] = features['final_train_acc'] - features['final_val_acc']\n",
    "    features['val_train_loss_diff'] = features['final_val_loss'] - features['final_train_loss']\n",
    "\n",
    "    return features\n",
    "\n",
    "# Main procedure to load, extract features, run PCA, and rank models\n",
    "def rank_models_with_pca(json_folder='metrics/'):\n",
    "    # 1. Load metrics & training histories\n",
    "    df_metrics = load_metrics(json_folder)\n",
    "\n",
    "    if df_metrics.empty:\n",
    "        print(\"No metric JSON files found in the given folder!\")\n",
    "        return None\n",
    "\n",
    "    # 2. Extract history-based features and combine with metrics\n",
    "    features_list = []\n",
    "    for idx, row in df_metrics.iterrows():\n",
    "        hist_features = extract_history_features(row['training_history'])\n",
    "        combined = {**row.to_dict(), **hist_features}\n",
    "        features_list.append(combined)\n",
    "\n",
    "    feature_df = pd.DataFrame(features_list)\n",
    "\n",
    "    # Drop non-numeric or identifier columns before scaling\n",
    "    drop_cols = ['model', '_file', 'training_history']\n",
    "    feature_df_clean = feature_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # 3. Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(feature_df_clean.fillna(0))\n",
    "\n",
    "    # 4. PCA: reduce to 1 component as composite performance score\n",
    "    pca = PCA(n_components=1)\n",
    "    pc1_scores = pca.fit_transform(X_scaled).flatten()\n",
    "\n",
    "    # 5. Add composite score and rank\n",
    "    df_metrics['composite_score'] = pc1_scores\n",
    "    ranked_df = df_metrics.sort_values('composite_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return ranked_df\n",
    "\n",
    "ranked_models = rank_models_with_pca(json_folder='metrics/')\n",
    "if ranked_models is not None:\n",
    "    display(ranked_models[['model', 'composite_score', 'test_accuracy', 'macro_f1', 'final_train_accuracy', 'best_val_accuracy']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22961e",
   "metadata": {},
   "source": [
    "## Advanced Model Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7e92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2b0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
